{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search\n",
        "Exhaustively searches all possible combinations of hyperparameters specified in a parameter grid.\n",
        "Deterministic: Always finds the same best parameters given the same data and grid.\n",
        "Can be computationally expensive, especially with a large parameter grid or dataset.\n",
        "Best when the search space is relatively small.\n",
        "\n",
        "Random Search\n",
        "Randomly samples hyperparameter combinations from a distribution over possible values.\n",
        "Non-deterministic: Can find different best parameters in different runs.\n",
        "Usually more efficient than grid search, particularly when the search space is large.\n",
        "More likely to find good hyperparameter combinations in less time, especially if some hyperparameters are less important than others.\n"
      ],
      "metadata": {
        "id": "C4Hkjxpf2IXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_n736W3CMyla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02376127-f014-408e-e9e9-ac3fae0beec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GridSearchCV best parameters: {'n_neighbors': np.int64(14), 'p': 1, 'weights': 'uniform'}\n",
            "GridSearchCV best accuracy: 0.9666666666666666\n",
            "\n",
            "RandomizedSearchCV best parameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': np.int64(3)}\n",
            "RandomizedSearchCV best accuracy: 0.9583333333333334\n",
            "\n",
            "GridSearchCV test accuracy: 1.0\n",
            "RandomizedSearchCV test accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_neighbors': np.arange(1, 31),  # Test neighbors from 1 to 30\n",
        "    'weights': ['uniform', 'distance'],  # Test different weighting schemes\n",
        "    'p': [1, 2]  # Test Manhattan and Euclidean distances\n",
        "}\n",
        "\n",
        "# Create a KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)  # Use all cores\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy for GridSearchCV\n",
        "print(\"GridSearchCV best parameters:\", grid_search.best_params_)\n",
        "print(\"GridSearchCV best accuracy:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_neighbors': np.arange(1, 31),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy for RandomizedSearchCV\n",
        "print(\"\\nRandomizedSearchCV best parameters:\", random_search.best_params_)\n",
        "print(\"RandomizedSearchCV best accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the best models on the test set (optional)\n",
        "print(\"\\nGridSearchCV test accuracy:\", grid_search.score(X_test, y_test))\n",
        "print(\"RandomizedSearchCV test accuracy:\", random_search.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "# Load your data (replace 'your_data.csv' with your actual file)\n",
        "try:\n",
        "    data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'your_data.csv' not found. Please upload your data file.\")\n",
        "    data = None\n",
        "\n",
        "# Split into features and target\n",
        "X = data.iloc[:, 1:]  # All columns except first\n",
        "y = data.iloc[:, 0]   # First column is target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "dUbEBNfd2Bhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1dea38-73ba-4f7c-b684-657d82e61b60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the MNIST data\n",
        "data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = data.iloc[:, 1:]  # Pixel values (columns 1-784)\n",
        "y = data.iloc[:, 0]   # Digit labels (column 0)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid_dt = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create and fit GridSearchCV\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nDecision Tree GridSearchCV results:\")\n",
        "print(\"Best parameters:\", grid_search_dt.best_params_)\n",
        "print(\"Best cross-validation accuracy: {:.4f}\".format(grid_search_dt.best_score_))\n",
        "print(\"Test set accuracy: {:.4f}\".format(grid_search_dt.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "s1SiGZei2UJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST data\n",
        "data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = data.iloc[:, 1:]  # Pixel values (columns 1-784)\n",
        "y = data.iloc[:, 0]   # Digit labels (column 0)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist_dt = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None] + list(np.arange(5, 101, 5)),  # From 5 to 100 in steps of 5\n",
        "    'min_samples_split': np.arange(2, 21),  # From 2 to 20\n",
        "    'min_samples_leaf': np.arange(1, 11),   # From 1 to 10\n",
        "    'max_features': [None, 'sqrt', 'log2', 0.5, 0.8],\n",
        "    'max_leaf_nodes': [None, 50, 100, 200, 500],\n",
        "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Create and fit RandomizedSearchCV\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "random_search_dt = RandomizedSearchCV(\n",
        "    dt,\n",
        "    param_distributions=param_dist_dt,\n",
        "    n_iter=100,               # Number of parameter settings sampled\n",
        "    cv=5,                     # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,                # Use all available cores\n",
        "    random_state=42,          # Reproducible results\n",
        "    verbose=1                 # Show progress\n",
        ")\n",
        "\n",
        "random_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nDecision Tree RandomizedSearchCV results:\")\n",
        "print(\"Best parameters:\", random_search_dt.best_params_)\n",
        "print(\"Best cross-validation accuracy: {:.4f}\".format(random_search_dt.best_score_))\n",
        "print(\"Test set accuracy: {:.4f}\".format(random_search_dt.score(X_test, y_test)))\n",
        "\n",
        "# Get feature importances from the best model\n",
        "best_dt = random_search_dt.best_estimator_\n",
        "feature_importances = best_dt.feature_importances_\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(feature_importances)), feature_importances)\n",
        "plt.title(\"Feature Importances from Decision Tree\")\n",
        "plt.xlabel(\"Pixel Index\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UIzM3HXqi8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load the MNIST data (no headers)\n",
        "try:\n",
        "    data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'mnist_train_small.csv' not found. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# Split into features (pixels) and target (digit labels)\n",
        "X = data.iloc[:, 1:].values  # All columns except first (pixel values)\n",
        "y = data.iloc[:, 0].values   # First column is the digit label (0-9)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train SVM classifier with timing\n",
        "print(\"Training SVM...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Using a linear kernel for MNIST (works well and is faster than RBF)\n",
        "clf = svm.SVC(kernel='linear', C=1.0, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nSVM Classifier Results:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Number of support vectors per class: {clf.n_support_}\")"
      ],
      "metadata": {
        "id": "CMUT5WDB9ijY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Load the MNIST data (no headers)\n",
        "try:\n",
        "    data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'mnist_train_small.csv' not found. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# Split into features (pixels) and target (digit labels)\n",
        "X = data.iloc[:, 1:]  # All columns except first (pixel values)\n",
        "y = data.iloc[:, 0]   # First column is the digit label (0-9)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with StandardScaler and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm.SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],  # Reduced from [0.1, 1, 10, 100] for faster search\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Removed 'poly' to reduce search space\n",
        "    'svm__gamma': ['scale', 'auto', 0.01, 0.1],  # Adjusted values\n",
        "    'svm__class_weight': [None, 'balanced']  # Added for imbalanced classes\n",
        "}\n",
        "\n",
        "print(\"Starting GridSearchCV...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform GridSearchCV with reduced n_jobs if memory is limited\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=3,  # Reduced from 5 for faster search\n",
        "    scoring='accuracy',\n",
        "    n_jobs=4,  # Reduced from -1 to prevent memory issues\n",
        "    verbose=2  # Shows progress\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"GridSearchCV completed in {time.time()-start_time:.2f} seconds\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nSVM GridSearchCV Results:\")\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test set accuracy: {grid_search.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Additional evaluation\n",
        "y_pred = best_svm.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0FaAAKrC_at7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "# Load the MNIST data (no headers)\n",
        "try:\n",
        "    data = pd.read_csv('./sample_data/mnist_train_small.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'mnist_train_small.csv' not found. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# Split into features (pixels) and target (digit labels)\n",
        "X = data.iloc[:, 1:]  # All columns except first (pixel values)\n",
        "y = data.iloc[:, 0]   # First column is the digit label (0-9)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                   test_size=0.2,\n",
        "                                                   random_state=42,\n",
        "                                                   stratify=y)  # Added stratification\n",
        "\n",
        "# Create a pipeline with StandardScaler and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm.SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist_svm = {\n",
        "    'svm__C': np.logspace(-3, 3, 7),  # 7 values from 0.001 to 1000\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svm__gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 5)),  # 7 options total\n",
        "    'svm__degree': [2, 3, 4],  # Only used with poly kernel\n",
        "    'svm__coef0': np.linspace(-1, 1, 5),  # Only used with poly/sigmoid\n",
        "    'svm__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "print(\"Starting RandomizedSearchCV...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search_svm = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist_svm,\n",
        "    n_iter=20,  # Increased from 10 to sample more combinations\n",
        "    cv=3,       # Reduced from 5 for faster search\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42,\n",
        "    verbose=2   # Show progress\n",
        ")\n",
        "\n",
        "random_search_svm.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nRandomizedSearchCV completed in {(time.time()-start_time)/60:.2f} minutes\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nSVM RandomizedSearchCV Results:\")\n",
        "print(\"Best parameters:\", random_search_svm.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {random_search_svm.best_score_:.4f}\")\n",
        "print(f\"Test set accuracy: {random_search_svm.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_svm = random_search_svm.best_estimator_\n",
        "\n",
        "# Additional evaluation\n",
        "y_pred = best_svm.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "T25pCS00AIPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}